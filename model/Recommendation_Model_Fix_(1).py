# -*- coding: utf-8 -*-
"""Recommendation Model Fix (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11UZ6ywn8CaiWhJTYZYaL2YVkoHq0BpzI
"""

# Commented out IPython magic to ensure Python compatibility.
#import library

#for data processing
import pandas as pd
import numpy as np
from zipfile import ZipFile
from pathlib import Path

#for data visualization
import seaborn as sns
import matplotlib.pyplot as plt

# %matplotlib inline
sns.set_palette('Set1')
sns.set()

#for modelling
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

#removing warning when plotting seaborn
import warnings
warnings.filterwarnings('ignore')

#for uploud file
import os

#data preparation

#saving each dataset into variable
rate = pd.read_csv('tourism_rating(1).csv')
place = pd.read_csv('tourism_with_id(1).csv', encoding="ISO-8859-1", error_bad_lines=False)
user = pd.read_csv('user.csv')

# DATA PLACE

#showing data place
place.head(5)

#remove unused column

place = place.drop(['Unnamed: 11','Unnamed: 12'],axis=1)
place.head(5)

place.info()

# remove unused column

place = place.drop('Time_Minutes', axis=1)
place.head(5)

# DATA RATING

# showing data rating
rate.head(5)

rate.info()

# Showing rating dataset large 

rate.shape

#DATA USER

# showing data user
user.head(5)

# Showing user dataset large

user.shape

#DATA EXPLORATION

# Create a dataframe containing the location with the highest number of ratings
top_10 = rate['Place_Id'].value_counts().reset_index()[0:10]
top_10 = pd.merge(top_10, place[['Place_Id','Place_Name']], how='left', left_on='index', right_on='Place_Id')

# Create a tourist visualization with the highest number of ratings
plt.figure(figsize=(8,5))
sns.barplot('Place_Id_x', 'Place_Name', data=top_10)
plt.title('Number of Tourist Attractions with the Most Ratings', pad=20)
plt.ylabel('Total Rating')
plt.xlabel('Location Name')
plt.show()

# Make a visualization of the number of tourism categories 

sns.countplot(y='Category', data=place)
plt.title('Comparison of the Number of Tourism Categories in the Central Java', pad=20)
plt.show()

# Create user age distribution visualization

plt.figure(figsize=(5,3))
sns.boxplot(user['Age']);
plt.title('User Age Distribution', pad=20)
plt.show()

# Make a visualization of the distribution of tourist attractions entrance prices

plt.figure(figsize=(7,3))
sns.boxplot(place['Price'])
plt.title('Distribution of Tour Entrance Prices in Central Java', pad=20)
plt.show()

# Filtering origin city from user
askot = user['Location'].apply(lambda x : x.split(',')[0])

# Visualization of the origin of the city from the user
plt.figure(figsize=(8,6))
sns.countplot(y=askot)
plt.title('Number of Origin City of User')
plt.show()

# DATA MODELLING PREPARATION

# create copy of data rating

# Read the dataset for encoding
df = rate.copy()
df.head()

# do encoding

# Creating Functions for Encoding
def dict_encoder(col, data=df):

  # Converting a column of a dataframe into a list without the same value
  unique_val = data[col].unique().tolist()

  # Encoding the column values of a dataframe to numbers
  val_to_val_encoded = {x: i for i, x in enumerate(unique_val)}

  # Perform the process of encoding numbers to values from columns of a dataframe
  val_encoded_to_val = {i: x for i, x in enumerate(unique_val)}
  return val_to_val_encoded, val_encoded_to_val

# Encoding and Mapping User column

# Encoding User_Id
user_to_user_encoded, user_encoded_to_user = dict_encoder('User_Id')

# Mapping User_Id to dataframe
df['user'] = df['User_Id'].map(user_to_user_encoded)

# Encoding and Mapping Place column

# Encoding Place_Id
place_to_place_encoded, place_encoded_to_place = dict_encoder('Place_Id')

# Mapping Place_Id to dataframe place
df['place'] = df['Place_Id'].map(place_to_place_encoded)

# Viewing Data Overview for Modeling

# Get total user and place
num_users, num_place = len(user_to_user_encoded), len(place_to_place_encoded)
 
# Convert rating to float value
df['Place_Ratings'] = df['Place_Ratings'].values.astype(np.float32)
 
# Get the minimum and maximum rating
min_rate, max_rate = min(df['Place_Ratings']), max(df['Place_Ratings'])
 
print(f'Number of User: {num_users}, Number of Place: {num_place}, Min Rating: {min_rate}, Max Rating: {max_rate}')

# Shuffle the dataset
df = df.sample(frac=1, random_state=42)
df.head(5)

# ML Modelling Using RecommenderNet

# Split train and test data

# Create a variable x to match user and place data into one value
x = df[['user', 'place']].values
 
# Create a variable y to make a rating of the results 
y = df['Place_Ratings'].apply(lambda x: (x - min_rate) / (max_rate - min_rate)).values
 
# Splits into 80% train data and 20% validation data
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

# Preparing Modell

class RecommenderNet(tf.keras.Model):
 
  # Function initialization
  def __init__(self, num_users, num_places, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_places = num_places
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.places_embedding = layers.Embedding( # layer embeddings places
        num_places,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.places_bias = layers.Embedding(num_places, 1) # layer embedding places bias
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # calling layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # calling layer embedding 2
    places_vector = self.places_embedding(inputs[:, 1]) # calling layer embedding 3
    places_bias = self.places_bias(inputs[:, 1]) # calling layer embedding 4
 
    dot_user_places = tf.tensordot(user_vector, places_vector, 2) 
 
    x = dot_user_places + user_bias + places_bias
    
    return tf.nn.sigmoid(x) # activation sigmoid

# Modell initialization
model = RecommenderNet(num_users, num_place, 50) 
 
# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.0004),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Callbacks initialization
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('val_root_mean_squared_error')<0.25):
      print('Validation metric is as expected!')
      self.model.stop_training = True

# Training Process

# Start training
 
history = model.fit(
    x = x_train,
    y = y_train,
    epochs = 100,
    validation_data = (x_val, y_val),
    callbacks = [myCallback()]
)

# Display loss plots and validation

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.ylim(ymin=0, ymax=0.4)
plt.legend(['train', 'test'], loc='center left')
plt.show()

# Predict top 5 Recommendation

# Setting Up a DataFrame to Display Recommended Results

# Preapring dataframe
place_df = place[['Place_Id','Place_Name','Category','Rating','Price']]
place_df.columns = ['id','place_name','category','rating','price']
df = rate.copy()

# Setting up User samples to Show Recommendations

# Looking for user
# Take sample user
user_id = df.User_Id.sample(1).iloc[0]
place_visited_by_user = df[df.User_Id == user_id]

# Searching for Locations that the User hasn't Visited

# Create location data that has not been visited by the user
place_not_visited = place_df[~place_df['id'].isin(place_visited_by_user.Place_Id.values)]['id'] 
place_not_visited = list(
    set(place_not_visited)
    .intersection(set(place_to_place_encoded.keys()))
)
 
place_not_visited = [[place_to_place_encoded.get(x)] for x in place_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_place_array = np.hstack(
    ([[user_encoder]] * len(place_not_visited), place_not_visited)
)

# Displaying Recommendation Results for Users

# Take top 5 recommendation
ratings = model.predict(user_place_array).flatten()
top_ratings_indices = ratings.argsort()[-5:][::-1]
recommended_place_ids = [
    place_encoded_to_place.get(place_not_visited[x][0]) for x in top_ratings_indices
]
 
print('List of Recommendation for: {}'.format('User ' + str(user_id)))
print('===' * 15,'\n')
print('----' * 15)
print('The place with the highest travel rating from users')
print('----' * 15)
 
top_place_user = (
    place_visited_by_user.sort_values(
        by = 'Place_Ratings',
        ascending=False
    )
    .head(5)
    .Place_Id.values
)
 
place_df_rows = place_df[place_df['id'].isin(top_place_user)]
for row in place_df_rows.itertuples():
    print(row.place_name, ':', row.category)

print('')
print('----' * 15)
print('Top 5 Recommendation')
print('----' * 15)
 
recommended_place = place_df[place_df['id'].isin(recommended_place_ids)]
for row, i in zip(recommended_place.itertuples(), range(1,8)):
    print(i,'.', row.place_name, '\n    ', row.category, ',', 'Price of admission : ', row.price, ',', 'Travel Rating : ', row.rating,'\n')

print('==='*15)

#generate a saved model

import pathlib
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input

export_dir = 'saved_model/1'
tf.saved_model.save(model, export_dir)

# Convert the model.
converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)
tflite_model = converter.convert()

tflite_model_file = pathlib.Path('model.tflite')
tflite_model_file.write_bytes(tflite_model)

# Load TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

try:
  from google.colab import files
  files.download(tflite_model_file)
except:
  pass

